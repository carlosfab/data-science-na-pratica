{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wEybrAx2fxo"
      },
      "source": [
        "<img alt=\"Colaboratory logo\" width=\"15%\" src=\"https://raw.githubusercontent.com/carlosfab/escola-data-science/master/img/novo_logo_bg_claro.png\">\n",
        "\n",
        "#### **Data Science na Prática**\n",
        "*by [sigmoidal.ai](https://sigmoidal.ai)*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "206Dc4JI2g4I"
      },
      "source": [
        "# Workflow para Machine Learning\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1533749871411-5e21e14bcc7d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1471&q=80\" width=\"60%\"></center>\n",
        "\n",
        "O *workflow* para problemas de Machine Learning apresentado pelo Google, é composto de 6 etapas principais:\n",
        "\n",
        "1. Adquirir os Dados\n",
        "2. Explorar os Dados\n",
        "3. Preparar os Dados\n",
        "4. Construir, Treinar e Avaliar seu Modelo\n",
        "5. Otimizar os Hiperparâmetros\n",
        "6. Deploy do Modelo\n",
        "\n",
        "Há ainda uma etapa `2.5 Escolher o Modelo`, que não faz parte dos *frameworks* tradicionais, mas que merece destaque devido a sua importância crítica.\n",
        "\n",
        "Para conhecer mais sobre esse workflow, visite o [guideline da Google](https://developers.google.com/machine-learning/guides/text-classification).\n",
        "\n",
        "<center><img src=\"https://raw.githubusercontent.com/carlosfab/dsnp2/master/img/Workflow-2.png\" width=\"700\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su7yLBTx0aHI"
      },
      "source": [
        "## 1. Aquisição dos dados\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1528582654826-585a71fcf7f4?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1470&q=80\" width=\"50%\"></center>\n",
        "\n",
        "De onde virão os seus dados? De um `csv`, `xls`, de um banco de dados `mysql`, consumindo microserviços, fazendo web scraping ou usando APIs disponibilizadas por empresas como Twitter, RD Station e Google?\n",
        "\n",
        "Se for por meio de APIs, quais são as regras para não ser bloqueado? O consumo de dados é gratuito? Precisa criar uma chave?\n",
        "\n",
        "Se for por meio das planilhas da sua empresa, você sabe se elas têm dependências, se alguém pode alterar a lógica ou corromper o arquivo? Os lançamentos são manuais, por meio de campos abertos, ou fechados?\n",
        "\n",
        "Os dados estão minimamente balanceados?\n",
        "\n",
        "As informações representam estatisticamente todas as situações possíveis?\n",
        "\n",
        "Depois de importar os dados, você realizou cheques para ver se eles estão consistentes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmmBdtzx0jYw"
      },
      "source": [
        "**Exemplo 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo304mWRzbHb"
      },
      "outputs": [],
      "source": [
        "def load_imdb_sentiment_analysis_dataset(data_path, seed=123):\n",
        "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        seed: int, seed for randomizer.\n",
        "\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 25000\n",
        "        Number of test samples: 25000\n",
        "        Number of categories: 2 (0 - negative, 1 - positive)\n",
        "\n",
        "    # References\n",
        "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
        "\n",
        "        Download and uncompress archive from:\n",
        "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "    \"\"\"\n",
        "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
        "\n",
        "    # Load the training data\n",
        "    train_texts = []\n",
        "    train_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
        "        for fname in sorted(os.listdir(train_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(train_path, fname)) as f:\n",
        "                    train_texts.append(f.read())\n",
        "                train_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Load the validation data.\n",
        "    test_texts = []\n",
        "    test_labels = []\n",
        "    for category in ['pos', 'neg']:\n",
        "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
        "        for fname in sorted(os.listdir(test_path)):\n",
        "            if fname.endswith('.txt'):\n",
        "                with open(os.path.join(test_path, fname)) as f:\n",
        "                    test_texts.append(f.read())\n",
        "                test_labels.append(0 if category == 'neg' else 1)\n",
        "\n",
        "    # Shuffle the training data and labels.\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_texts)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(train_labels)\n",
        "\n",
        "    return ((train_texts, np.array(train_labels)),\n",
        "            (test_texts, np.array(test_labels)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u0B-XDz0Pyh"
      },
      "source": [
        "**Exemplo 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiipD04yBvrD"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVLrfWf0Bwlu"
      },
      "source": [
        "**Exemplo 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvARi61n0n2O"
      },
      "outputs": [],
      "source": [
        "# USAGE\n",
        "# python build_lisa_records.py\n",
        "\n",
        "# import the necessary packages\n",
        "from config import lisa_config as config\n",
        "from pyimagesearch.utils.tfannotation import TFAnnotation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "def main(_):\n",
        "\t# open the classes output file\n",
        "\tf = open(config.CLASSES_FILE, \"w\")\n",
        "\n",
        "\t# loop over the classes\n",
        "\tfor (k, v) in config.CLASSES.items():\n",
        "\t\t# construct the class information and write to file\n",
        "\t\titem = (\"item {\\n\"\n",
        "\t\t\t\t\"\\tid: \" + str(v) + \"\\n\"\n",
        "\t\t\t\t\"\\tname: '\" + k + \"'\\n\"\n",
        "\t\t\t\t\"}\\n\")\n",
        "\t\tf.write(item)\n",
        "\n",
        "\t# close the output classes file\n",
        "\tf.close()\n",
        "\n",
        "\t# initialize a data dictionary used to map each image filename\n",
        "\t# to all bounding boxes associated with the image, then load\n",
        "\t# the contents of the annotations file\n",
        "\tD = {}\n",
        "\trows = open(config.ANNOT_PATH).read().strip().split(\"\\n\")\n",
        "\n",
        "\t# loop over the individual rows, skipping the header\n",
        "\tfor row in rows[1:]:\n",
        "\t\t# break the row into components\n",
        "\t\trow = row.split(\",\")[0].split(\";\")\n",
        "\t\t(imagePath, label, startX, startY, endX, endY, _) = row\n",
        "\t\t(startX, startY) = (float(startX), float(startY))\n",
        "\t\t(endX, endY) = (float(endX), float(endY))\n",
        "\n",
        "\t\t# if we are not interested in the label, ignore it\n",
        "\t\tif label not in config.CLASSES:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\t# build the path to the input image, then grab any other\n",
        "\t\t# bounding boxes + labels associated with the image\n",
        "\t\t# path, labels, and bounding box lists, respectively\n",
        "\t\tp = os.path.sep.join([config.BASE_PATH, imagePath])\n",
        "\t\tb = D.get(p, [])\n",
        "\n",
        "\t\t# build a tuple consisting of the label and bounding box,\n",
        "\t\t# then update the list and store it in the dictionary\n",
        "\t\tb.append((label, (startX, startY, endX, endY)))\n",
        "\t\tD[p] = b\n",
        "\n",
        "\t# create training and testing splits from our data dictionary\n",
        "\t(trainKeys, testKeys) = train_test_split(list(D.keys()),\n",
        "\t\ttest_size=config.TEST_SIZE, random_state=42)\n",
        "\n",
        "\t# initialize the data split files\n",
        "\tdatasets = [\n",
        "\t\t(\"train\", trainKeys, config.TRAIN_RECORD),\n",
        "\t\t(\"test\", testKeys, config.TEST_RECORD)\n",
        "\t]\n",
        "\n",
        "\t# loop over the datasets\n",
        "\tfor (dType, keys, outputPath) in datasets:\n",
        "\t\t# initialize the TensorFlow writer and initialize the total\n",
        "\t\t# number of examples written to file\n",
        "\t\tprint(\"[INFO] processing '{}'...\".format(dType))\n",
        "\t\twriter = tf.python_io.TFRecordWriter(outputPath)\n",
        "\t\ttotal = 0\n",
        "\n",
        "\t\t# loop over all the keys in the current set\n",
        "\t\tfor k in keys:\n",
        "\t\t\t# load the input image from disk as a TensorFlow object\n",
        "\t\t\tencoded = tf.gfile.GFile(k, \"rb\").read()\n",
        "\t\t\tencoded = bytes(encoded)\n",
        "\n",
        "\t\t\t# load the image from disk again, this time as a PIL\n",
        "\t\t\t# object\n",
        "\t\t\tpilImage = Image.open(k)\n",
        "\t\t\t(w, h) = pilImage.size[:2]\n",
        "\n",
        "\t\t\t# parse the filename and encoding from the input path\n",
        "\t\t\tfilename = k.split(os.path.sep)[-1]\n",
        "\t\t\tencoding = filename[filename.rfind(\".\") + 1:]\n",
        "\n",
        "\t\t\t# initialize the annotation object used to store\n",
        "\t\t\t# information regarding the bounding box + labels\n",
        "\t\t\ttfAnnot = TFAnnotation()\n",
        "\t\t\ttfAnnot.image = encoded\n",
        "\t\t\ttfAnnot.encoding = encoding\n",
        "\t\t\ttfAnnot.filename = filename\n",
        "\t\t\ttfAnnot.width = w\n",
        "\t\t\ttfAnnot.height = h\n",
        "\n",
        "\t\t\t# loop over the bounding boxes + labels associated with\n",
        "\t\t\t# the image\n",
        "\t\t\tfor (label, (startX, startY, endX, endY)) in D[k]:\n",
        "\t\t\t\t# TensorFlow assumes all bounding boxes are in the\n",
        "\t\t\t\t# range [0, 1] so we need to scale them\n",
        "\t\t\t\txMin = startX / w\n",
        "\t\t\t\txMax = endX / w\n",
        "\t\t\t\tyMin = startY / h\n",
        "\t\t\t\tyMax = endY / h\n",
        "\n",
        "\t\t\t\t# update the bounding boxes + labels lists\n",
        "\t\t\t\ttfAnnot.xMins.append(xMin)\n",
        "\t\t\t\ttfAnnot.xMaxs.append(xMax)\n",
        "\t\t\t\ttfAnnot.yMins.append(yMin)\n",
        "\t\t\t\ttfAnnot.yMaxs.append(yMax)\n",
        "\t\t\t\ttfAnnot.textLabels.append(label.encode(\"utf8\"))\n",
        "\t\t\t\ttfAnnot.classes.append(config.CLASSES[label])\n",
        "\t\t\t\ttfAnnot.difficult.append(0)\n",
        "\n",
        "\t\t\t\t# increment the total number of examples\n",
        "\t\t\t\ttotal += 1\n",
        "\n",
        "\t\t\t# encode the data point attributes using the TensorFlow\n",
        "\t\t\t# helper functions\n",
        "\t\t\tfeatures = tf.train.Features(feature=tfAnnot.build())\n",
        "\t\t\texample = tf.train.Example(features=features)\n",
        "\n",
        "\t\t\t# add the example to the writer\n",
        "\t\t\twriter.write(example.SerializeToString())\n",
        "\n",
        "\t\t# close the writer and print diagnostic information to the\n",
        "\t\t# user\n",
        "\t\twriter.close()\n",
        "\t\tprint(\"[INFO] {} examples saved for '{}'\".format(total,\n",
        "\t\t\tdType))\n",
        "\n",
        "# check to see if the main thread should be started\n",
        "if __name__ == \"__main__\":\n",
        "\ttf.app.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wyzJiDS0rrG"
      },
      "source": [
        "## 2. Explorar os dados\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1614314266357-8a2e58059af5?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1021&q=80\" width=\"50%\"></center>\n",
        "\n",
        "Entender os dados é a fase que permite você entender qual a melhor abordagem, melhor modelo, melhores features para serem usadas.\n",
        "\n",
        "Todos os insights devem ser anotados e documentados da melhor maneira possível. Alguns questionamentos a serem feitos são:\n",
        "\n",
        "* Qual o número de amostras?\n",
        "* Qual o número total de classes?\n",
        "* O dataset está balanceado?\n",
        "* Quais variáveis são categóricas ou numéricas?\n",
        "* Qual a porcentagem de valores ausentes?\n",
        "* Há outliers presentes?\n",
        "* Qual o tipo de distribuição das suas variáveis?\n",
        "* Qual a variável alvo?\n",
        "* Existe correlação entre as variáveis?\n",
        "* Quais transformações são aplicáveis, começando a raciocinar com o nosso modelo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n_-A2jQN2y7"
      },
      "source": [
        "## 2.5 Escolher o Modelo\n",
        "\n",
        "<center><img src=\"https://www.incimages.com/uploaded_files/image/1920x1080/getty_178784049_9706479704500197_55273.jpg\" width=\"60%\"></center>\n",
        "\n",
        "\n",
        "Baseado nas observações e métricas identificadas nas etapas anteriores, você começa a ter uma ideia de qual modelo seria mais adequado ao problema que você está atacando.\n",
        "\n",
        "Obviamente, depende muito da experiência e conhecimento, mas existem alguns guias que ajudam na escolha do algoritmo.\n",
        "\n",
        "Veja o material que está em anexo na aula."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMZwIZY9DDU"
      },
      "source": [
        "# 3 Preparação dos Dados\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1571666521805-f5e8423aba9d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1470&q=80\" width=\"55%\"></center>\n",
        "\n",
        "Existem várias transformações que podem ser possíveis, mas às vezes fica confuso quais devemos fazer. Gosto de dividir entre transformações obrigatórias e transformações opcionais:\n",
        "\n",
        "1. Transformações obrigatórias\n",
        "    * Converter dados não numéricos em numéricos, pois você não vai conseguir realizar operações vetorizadas sem converter strings, por exemplo, em variáveis numéricas.\n",
        "    * Redimensionar inputs para valores fixos, pois o seu modelo espera ser alimentado sempre da mesma maneira.\n",
        "    * Lidar com os valores ausentes, seja excluindo as entradas ou substituindo seus valores\n",
        "    * Remoção de outliers, uma vez que eles irão dar um viés ao nosso modelo.\n",
        "2. Transformações opcionais\n",
        "    * Normalizar ou padronizar as features, dependendo do modelo e demandas específicas.\n",
        "    * Tratamento adequado de strings, como deixar todos os caracteres em minúsculo.\n",
        "    * Criar um fluxo para deixar o dataset sempre anônimo.\n",
        "    * Reduzir a dimensionalidade do problema, dependendo da quantidade e qualidade das suas features.\n",
        "    * Criar novas variáveis, buscando técnicas de feature engineering.\n",
        "\n",
        "Os dados podem ser preparados antes ou dentro do modelo. Veja as vantagens e desvantagens de cada uma dessas abordagens [neste link](https://developers.google.com/machine-learning/data-prep/transform/introduction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXJWyqB0fSVV"
      },
      "source": [
        "# 4 Construir, Treinar e Avaliar o seu modelo\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1587654780291-39c9404d746b?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1470&q=80\" width=\"55%\"></center>\n",
        "\n",
        "Você pode construir um modelo baseline, ou seja, um modelo bem básico para ser o seu parâmetro de desempenho.\n",
        "\n",
        "É comum muitos cientistas de dados pegarem apenas as variáveis numéricas e não se preocuparem com feature engineering ou codificação de variáveis categóricas. Como eu disse, um baseline é opcional, mas muito bem-vindo.\n",
        "\n",
        "Você deve entender as melhores métricas desde já, para realizar testes com vários modelos diferentes. O desempenho deles pode ser inferido pelo cross-validation.\n",
        "\n",
        "O modelo de melhor desempenho, de acordo com o cross-validation, será escolhido e poderemos melhorar ainda mais o desempenho dele.\n",
        "\n",
        "Detalhe, esse modelo final deve ser treinado em cima do dataset de treino completo. Como ele ainda não teve contato com os dados de teste, a avaliação final será feita com eles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuwpTK4Bn4BM"
      },
      "source": [
        "# 5 Otimizar os Hiperparâmetros\n",
        "\n",
        "<center><img src=\"https://images.unsplash.com/photo-1628666173519-66351692f747?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1074&q=80\" width=\"55%\"></center>\n",
        "\n",
        "\n",
        "Cada modelo tem a sua especificidade. Você deve conhecer cada parâmetro e o quanto ele impacta no desempenho e nas variáveis.\n",
        "\n",
        "Um modelo de Regressão Logística, por exemplo, tem muito pouca coisa a ser feito nesta etapa. Já um modelo classificador XGBoost, precisa que você siga determinadas etapas de otimização, a fim de não resultar em overfitting ou piora no desempenho.\n",
        "\n",
        "Hyperparameters tunning é uma arte, porém com método. Não use os dados de teste e trabalhe com Grid Search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGCBnnso5nAt"
      },
      "source": [
        "# 6 Deploy do Modelo\n",
        "\n",
        "<center><img src=\"https://blog-geek-midia.s3.amazonaws.com/wp-content/uploads/2020/11/11154014/o-que-e-deploy-e1605120444248.png\" width=\"55%\"></center>\n",
        "\n",
        "\n",
        "Deploy significar colocar o modelo em produção. Isso pode ser tão simples como disponibilizar uma API para ser consumida (que retorna as previsões do seu modelo), ou um sistema completo.\n",
        "\n",
        "Tenha em mente os seguintes pontos, importantes para o deploy:\n",
        "\n",
        "* Os seus novos dados devem ter a mesma distribuição, aquele usado nos dados de treino e validação.\n",
        "* Reveja o seu pipeline e identifique quais pré-processamentos foram feitos.\n",
        "* Reavalie o modelo constantemente. Decida se é necessário retreinar. Caso seja, mude a distribuição que será aplicada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xduxC-IIDaPy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
